{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phrase Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMI8CzzMNDzP9wGCH7/a3wP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASIF-Mahmud1/Exploration/blob/text-classifier/Phrase_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_5yOimx0nGx"
      },
      "source": [
        "**Note**: Copy Files from this link and put it in the root \n",
        "Link https://drive.google.com/drive/folders/1P58c2dz4N62b17JhGxBoYQnj-cmvFRlS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtipPGe2ctYr"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1GrKOzycd1C"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext, SQLContext\n",
        "from pyspark.broadcast import _broadcastRegistry\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lSevsHMdISZ"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "import time\n",
        "import config\n",
        "\n",
        "from operator import add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFuX8_Mtg-VO"
      },
      "source": [
        "!python3 \"config.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO6P4PbefW1b"
      },
      "source": [
        "logging.basicConfig(\n",
        "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "    level=logging.INFO)\n",
        "\n",
        "# Settings\n",
        "datafile = \"reviews_data.txt.gz\"\n",
        "output_file = config.phrase_generator['output-folder']\n",
        "phrases_file = config.phrase_generator['phrase-file']\n",
        "pre_tag = config.phrase_generator['dotag']\n",
        "stopfile = config.phrase_generator['stop-file']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuw4Bu9SeiLn"
      },
      "source": [
        " char_splitter = re.compile(\"[.,;!:()-]\")\n",
        "abspath = os.path.abspath(os.path.dirname(\"stopwords-en.txt\"))\n",
        "\n",
        " def load_stop_words():\n",
        "\n",
        "    return set(line.strip() for line in open(os.path.join(abspath, stopfile)))\n",
        "\n",
        " stopwords = load_stop_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anhzXj3bgb96"
      },
      "source": [
        "#stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmaVqQTsgm2o"
      },
      "source": [
        "def phrase_to_counts(phrases):\n",
        "    \"\"\" strip any white space and send back a count of 1\"\"\"\n",
        "    clean_phrases = []\n",
        "\n",
        "    for p in phrases:\n",
        "        word = p.strip()\n",
        "\n",
        "        # we only need to count phrases, so ignore unigrams\n",
        "        if len(word) > 1 and ' ' in word:\n",
        "            clean_phrases.append([word, 1])\n",
        "\n",
        "    return clean_phrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZUhkV3kF0O"
      },
      "source": [
        "def remove_special_characters(text):\n",
        "    \"\"\"remove characters that are not indicators of phrase boundaries\"\"\"\n",
        "    return re.sub(\"([{}@\\\"$%&\\\\\\/*'\\\"]|\\d)\", \"\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa1lcXlPkL2g"
      },
      "source": [
        "def generate_candidate_phrases(text, stopwords):\n",
        "    \"\"\" generate phrases using phrase boundary markers \"\"\"\n",
        "\n",
        "    # generate approximate phrases with punctation\n",
        "    coarse_candidates = char_splitter.split(text.lower())\n",
        "\n",
        "    candidate_phrases = []\n",
        "\n",
        "    for coarse_phrase\\\n",
        "            in coarse_candidates:\n",
        "\n",
        "        words = re.split(\"\\\\s+\", coarse_phrase)\n",
        "        previous_stop = False\n",
        "\n",
        "        # examine each word to determine if it is a phrase boundary marker or\n",
        "        # part of a phrase or lone ranger\n",
        "        for w in words:\n",
        "\n",
        "            if w in stopwords and not previous_stop:\n",
        "                # phrase boundary encountered, so put a hard indicator\n",
        "                candidate_phrases.append(\";\")\n",
        "                previous_stop = True\n",
        "            elif w not in stopwords and len(w) > 3:\n",
        "                # keep adding words to list until a phrase boundary is detected\n",
        "                candidate_phrases.append(w.strip())\n",
        "                previous_stop = False\n",
        "\n",
        "    # get a list of candidate phrases without boundary demarcation\n",
        "    phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
        "\n",
        "    return phrases\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzY3IVRlkYqK"
      },
      "source": [
        "def generate_and_tag_phrases(text_rdd, min_phrase_count=50):\n",
        "    \"\"\"Find top phrases, tag corpora with those top phrases\"\"\"\n",
        "\n",
        "    # load stop words for phrase boundary marking\n",
        "    logging.info(\"Loading stop words...\")\n",
        "    stopwords = load_stop_words()\n",
        "\n",
        "    # get top phrases with counts > 50\n",
        "    logging.info(\"Generating and collecting top phrases...\")\n",
        "    top_phrases_rdd = \\\n",
        "        text_rdd.map(lambda txt: remove_special_characters(txt))\\\n",
        "        .map(lambda txt: generate_candidate_phrases(txt, stopwords)) \\\n",
        "        .flatMap(lambda phrases: phrase_to_counts(phrases)) \\\n",
        "        .reduceByKey(add) \\\n",
        "        .sortBy(lambda phrases: phrases[1], ascending=False) \\\n",
        "        .filter(lambda phrases: phrases[1] >= min_phrase_count) \\\n",
        "        .sortBy(lambda phrases: phrases[0], ascending=True) \\\n",
        "        .map(lambda phrases: (phrases[0], phrases[0].replace(\" \", \"_\")))\n",
        "\n",
        "    shortlisted_phrases = top_phrases_rdd.collectAsMap()\n",
        "    logging.info(\"Done with phrase generation...\")\n",
        "\n",
        "    # write phrases to file which you can use down the road to tag your text\n",
        "    logging.info(\"Saving top phrases to {0}\".format(phrases_file))\n",
        "    with open(os.path.join(abspath, phrases_file), \"w\") as f:\n",
        "        for phrase in shortlisted_phrases:\n",
        "            f.write(phrase)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    # broadcast a few values so that these are not copied to the worker nodes\n",
        "    # each time\n",
        "    shortlisted_phrases_bc = sc.broadcast(shortlisted_phrases)\n",
        "    keys = list(shortlisted_phrases.keys())\n",
        "    keys.sort(key=len, reverse=True)\n",
        "    sorted_key_bc = sc.broadcast(keys)  # sorts by descending length\n",
        "    # tag corpora and save as new corpora\n",
        "    logging.info(\"Tagging corpora with phrases...this will take a while\")\n",
        "    tagged_text_rdd = text_rdd.map(\n",
        "        lambda txt: tag_data(\n",
        "            txt,\n",
        "            shortlisted_phrases_bc.value, sorted_key_bc.value))\n",
        "\n",
        "    return tagged_text_rdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z85TZF3NlWNk"
      },
      "source": [
        "def tag_data(original_text, phrase_transformation, keys):\n",
        "    \"\"\"Process the pipe separated file\"\"\"\n",
        "    original_text = original_text.lower()\n",
        "\n",
        "    # greedy approach, start with the longest phrase\n",
        "    for phrase in keys:\n",
        "        # keep track of all the substitutes for a given phrase\n",
        "        original_text = original_text.replace(\n",
        "            phrase, phrase_transformation[phrase])\n",
        "\n",
        "    return original_text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Create a spark configuration with 20 threads.\n",
        "    # This code will run locally on master\n",
        "    conf = (SparkConf()\n",
        "            . setAppName(\"sample app for reading files\"))\n",
        "\n",
        "    sc = SparkContext(conf=conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUqXsoYvlfMx"
      },
      "source": [
        "    # read text file, assumption here is that one document or sentences per line\n",
        "    # if you have a json file or other formats to read, you will have to\n",
        "    # change this a bit\n",
        "    text_rdd = sc.textFile(os.path.join(abspath, datafile)).repartition(10)\n",
        "\n",
        "    # generate candidate phrases and tag corpora with phrases\n",
        "    tagged_rdd = generate_and_tag_phrases(\n",
        "        text_rdd, min_phrase_count=config.phrase_generator['min-phrase-count'])\n",
        "\n",
        "    # save data as a new corpora\n",
        "    tagged_rdd.saveAsTextFile(\n",
        "        output_file,\n",
        "        \"org.apache.hadoop.io.compress.GzipCodec\")\n",
        "\n",
        "    logging.info(\n",
        "        \"Done! You can find your phrases here {0} and tagged corpora here {1}\".format(\n",
        "            phrases_file, output_file))\n",
        "\n",
        "    end = time.time()\n",
        "    time_taken = round(((end - start) / 60), 2)\n",
        "    print(\"Took {0} minutes to complete\".format(time_taken))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}