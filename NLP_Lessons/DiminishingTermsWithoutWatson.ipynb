{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiminishingTermsWithoutWatson.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKnaO3BlVtWw1RKp3QOLxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASIF-Mahmud1/Exploration/blob/text-classifier/NLP_Lessons/DiminishingTermsWithoutWatson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "C1ExT3zemNHh",
        "outputId": "38747c4a-8c9b-4b14-bfcd-2a280a4309d8"
      },
      "source": [
        "# evaluate multinomial logistic regression model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/ASIF-Mahmud1/Exploration/diminishingTerms/DiminishingTerms/dataSet.csv\"\n",
        "from io import StringIO\n",
        "import string\n",
        "import pandas as pd\n",
        "import requests\n",
        "s=requests.get(url).text\n",
        "\n",
        "message_data=pd.read_csv(StringIO(s))\n",
        "message_data['tag'] = message_data['tag'].str.strip()\n",
        "# message_data['sentence'] = message_data['sentence'].str.strip()\n",
        "message_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>diminishing</td>\n",
              "      <td>I�m no expert but �</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>strong</td>\n",
              "      <td>This is what I see</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>diminishing</td>\n",
              "      <td>It�s just my opinion �</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>strong</td>\n",
              "      <td>It�s my opinion�</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>diminishing</td>\n",
              "      <td>Just checking in �</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id          tag                sentence\n",
              "0   1  diminishing     I�m no expert but �\n",
              "1   2       strong      This is what I see\n",
              "2   3  diminishing  It�s just my opinion �\n",
              "3   4       strong        It�s my opinion�\n",
              "4   5  diminishing      Just checking in �"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gmQ3pVommID",
        "outputId": "56041727-b91d-4c29-8c7c-06d6737aa93b"
      },
      "source": [
        "message_data_copy = message_data['sentence'].copy() \n",
        "\n",
        "message_data_copy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                        I�m no expert but �\n",
              "1                         This is what I see\n",
              "2                     It�s just my opinion �\n",
              "3                           It�s my opinion�\n",
              "4                         Just checking in �\n",
              "                       ...                  \n",
              "71               Sorry for the inconvinience\n",
              "72    Sorry about the dog in the background�\n",
              "73                    Sorry if I look tired�\n",
              "74                     Sorry, I didn�t know \n",
              "75             Thank you for the information\n",
              "Name: sentence, Length: 76, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIR0O-edmpPH",
        "outputId": "18d280a0-bee5-4d3c-b75b-625ce608bbb1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ULaWPI9muGj"
      },
      "source": [
        "def text_preprocess(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
        "    return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39sw6frEm_pz"
      },
      "source": [
        "def stemmer (text):\n",
        "    text = text.split()\n",
        "    words = \"\"\n",
        "    for i in text:\n",
        "            stemmer = SnowballStemmer(\"english\")\n",
        "            words += (stemmer.stem(i))+\" \"\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz8S5LJbnB5q"
      },
      "source": [
        "message_data_copy = message_data_copy.apply(text_preprocess)\n",
        "message_data_copy = message_data_copy.apply(stemmer)    # New Line\n",
        "# message_data_copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_GhNEFjnFxD"
      },
      "source": [
        "vectorizer = TfidfVectorizer(\"english\")\n",
        "message_mat = vectorizer.fit_transform(message_data_copy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFtBlo-_nKqT"
      },
      "source": [
        "message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(message_mat, \n",
        "                                                        message_data['tag'], random_state=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGwZ8ps9nUi5",
        "outputId": "1c22a38a-67d4-4720-b069-39674945e861"
      },
      "source": [
        "Spam_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "Spam_model.fit(message_train, spam_nospam_train)\n",
        "pred = Spam_model.predict(message_test)\n",
        "print(\"The prediction is :\", pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction is : ['diminishing ' 'strong' 'strong' 'strong' 'diminishing ' 'strong'\n",
            " 'strong' 'strong' 'strong' 'diminishing ' 'diminishing ' 'diminishing '\n",
            " 'strong' 'strong' 'strong' 'diminishing ' 'strong' 'strong' 'strong']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPzdYamkXKpZ",
        "outputId": "4331e830-288b-4a66-e540-5b0017dcfee6"
      },
      "source": [
        "dummyEmail = [\" I understand \"]\n",
        "dummyEmailMatrix= vectorizer.transform(dummyEmail) \n",
        "dummyEmailMatrix\n",
        "predict= Spam_model.predict(dummyEmailMatrix)\n",
        "print(\"The prediction is :\", predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction is : ['strong']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyoGNQGTn9Do"
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz0PKAOeoJDx"
      },
      "source": [
        "# define the stages of the pipeline\n",
        "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
        "                                                      max_features=1000,\n",
        "                                                      stop_words=stopwords.words('english') )),\n",
        "                            ('model',  LogisticRegression(multi_class='multinomial', solver='lbfgs'))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q7qFESyoNt9",
        "outputId": "b294c267-16ff-4cf7-db77-c9a6f9f5ce2f"
      },
      "source": [
        "# fit the pipeline model with the training data                            \n",
        "pipeline.fit(message_data_copy,  message_data['tag'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=1000,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
              "                                             'our', 'ours', 'ourselves', 'you...\n",
              "                                 strip_accents=None, sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('model',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='multinomial', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53xmOakaoRoB",
        "outputId": "c72ef771-3850-4ef9-ab00-3f0d29337cfe"
      },
      "source": [
        "text = [\"I am hopeless at\", \"Sorry, I got it wrong\"]\n",
        "\n",
        "# predict the label using the pipeline\n",
        "pipeline.predict(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['diminishing ', 'diminishing '], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S2ApO2SoU8w",
        "outputId": "48b57e1f-2262-4303-b40c-555deb36e7af"
      },
      "source": [
        "prob_Of_Each_Class = Spam_model.predict_proba(dummyEmailMatrix)\n",
        "# summarize the predicted probabilities\n",
        "print('Predicted Probabilities: %s' % prob_Of_Each_Class[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Probabilities: [0.48960909 0.49283887 0.01755204]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}