{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ASIF-Mahmud1/Exploration/blob/text-classifier/NLP_Lessons/DiminishingTermsWithoutWatson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall --yes watson-machine-learning-client-V4\n",
    "!pip install watson-machine-learning-client-V4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "C1ExT3zemNHh",
    "outputId": "94b5a9de-5f9b-4398-f541-5a9e119e437e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>diminishing</td>\n",
       "      <td>I�m no expert but �</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>strong</td>\n",
       "      <td>This is what I see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>diminishing</td>\n",
       "      <td>It�s just my opinion �</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>strong</td>\n",
       "      <td>It�s my opinion�</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>diminishing</td>\n",
       "      <td>Just checking in �</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id          tag                sentence\n",
       "0   1  diminishing     I�m no expert but �\n",
       "1   2       strong      This is what I see\n",
       "2   3  diminishing  It�s just my opinion �\n",
       "3   4       strong        It�s my opinion�\n",
       "4   5  diminishing      Just checking in �"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate multinomial logistic regression model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/ASIF-Mahmud1/Exploration/diminishingTerms/DiminishingTerms/dataSet.csv\"\n",
    "from io import StringIO\n",
    "import string\n",
    "import pandas as pd\n",
    "import requests\n",
    "s=requests.get(url).text\n",
    "\n",
    "message_data=pd.read_csv(StringIO(s))\n",
    "message_data['tag'] = message_data['tag'].str.strip()\n",
    "# message_data['sentence'] = message_data['sentence'].str.strip()\n",
    "message_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gmQ3pVommID",
    "outputId": "b4831b6e-df53-41fd-ac75-dca1aadef739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        I�m no expert but �\n",
       "1                         This is what I see\n",
       "2                     It�s just my opinion �\n",
       "3                           It�s my opinion�\n",
       "4                         Just checking in �\n",
       "                       ...                  \n",
       "71               Sorry for the inconvinience\n",
       "72    Sorry about the dog in the background�\n",
       "73                    Sorry if I look tired�\n",
       "74                     Sorry, I didn�t know \n",
       "75             Thank you for the information\n",
       "Name: sentence, Length: 76, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data_copy = message_data['sentence'].copy() \n",
    "\n",
    "message_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIR0O-edmpPH",
    "outputId": "e571cb7a-dbb3-43fc-8815-fdbbfc952289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0ULaWPI9muGj"
   },
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "39sw6frEm_pz"
   },
   "outputs": [],
   "source": [
    "def stemmer (text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Jz8S5LJbnB5q"
   },
   "outputs": [],
   "source": [
    "message_data_copy = message_data_copy.apply(text_preprocess)\n",
    "message_data_copy = message_data_copy.apply(stemmer)    # New Line\n",
    "# message_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b_GhNEFjnFxD"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\"english\")\n",
    "message_mat = vectorizer.fit_transform(message_data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oFtBlo-_nKqT"
   },
   "outputs": [],
   "source": [
    "message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(message_mat, \n",
    "                                                        message_data['tag'], random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGwZ8ps9nUi5",
    "outputId": "ab594e0c-4884-49f5-e761-6b24106864ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is : ['diminishing' 'strong' 'strong' 'strong' 'diminishing' 'strong' 'strong'\n",
      " 'strong' 'strong' 'diminishing' 'diminishing' 'diminishing' 'strong'\n",
      " 'strong' 'strong' 'diminishing' 'strong' 'strong' 'strong']\n"
     ]
    }
   ],
   "source": [
    "Spam_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "Spam_model.fit(message_train, spam_nospam_train)\n",
    "pred = Spam_model.predict(message_test)\n",
    "print(\"The prediction is :\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPzdYamkXKpZ",
    "outputId": "5fdef354-d6e0-4bd7-dbe0-a65cea15154d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction is : ['strong']\n"
     ]
    }
   ],
   "source": [
    "dummyEmail = [\" I understand \"]\n",
    "dummyEmailMatrix= vectorizer.transform(dummyEmail) \n",
    "dummyEmailMatrix\n",
    "predict= Spam_model.predict(dummyEmailMatrix)\n",
    "print(\"The prediction is :\", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TyoGNQGTn9Do"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Bz0PKAOeoJDx"
   },
   "outputs": [],
   "source": [
    "# define the stages of the pipeline\n",
    "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                                      max_features=1000,\n",
    "                                                      stop_words=stopwords.words('english') )),\n",
    "                            ('model',  LogisticRegression(multi_class='multinomial', solver='lbfgs'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7q7qFESyoNt9",
    "outputId": "daaf3c56-0875-421e-b347-09788b6ab672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=1000,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you...\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='multinomial', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline model with the training data                            \n",
    "pipeline.fit(message_data_copy,  message_data['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53xmOakaoRoB",
    "outputId": "e753e32d-531f-4b90-a370-e927bde6c2ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['strong', 'diminishing'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"Do it now\", \"Sorry, I got it wrong\"]\n",
    "\n",
    "# predict the label using the pipeline\n",
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E395S6svfB3q",
    "outputId": "8fa22e01-1e65-45d5-c0da-4d362d71da86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in order  ['diminishing' 'strong']\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes in order \",pipeline.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqhO3dVVRYVm",
    "outputId": "f0e0fa89-7089-4653-a9d2-d51fc1e5b77e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities: [[0.47135431 0.52864569]\n",
      " [0.69215697 0.30784303]]\n"
     ]
    }
   ],
   "source": [
    "prob_Of_Each_Class = pipeline.predict_proba(text)\n",
    "print('Predicted Probabilities: %s' % prob_Of_Each_Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCOe1oqHWl2a",
    "outputId": "661a6c8b-7f6d-42b6-c8f5-9f21cfe297ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['diminishing', 'strong'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-S2ApO2SoU8w"
   },
   "outputs": [],
   "source": [
    "# prob_Of_Each_Class = Spam_model.predict_proba(dummyEmailMatrix)\n",
    "# # summarize the predicted probabilities\n",
    "# print('Predicted Probabilities: %s' % prob_Of_Each_Class[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPseywvGPj+WRZg0Cpkkuc0",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DiminishingTermsWithoutWatson.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
